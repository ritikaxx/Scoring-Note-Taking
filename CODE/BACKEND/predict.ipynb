{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from ast import literal_eval\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: \n",
    "A patient is identified who is to be assessed. The features (symptoms and conditions) evident and present in the patient are noted down.\n",
    "\n",
    "\n",
    "Step 2:\n",
    " The doctor interacts with the patient by asking them questions related to the conditions and making notes of the same.\n",
    "\n",
    "Step 3: \n",
    "The doctor’s notes along with the identified features in the patient are entered into an excel sheet (.csv file)\n",
    "\n",
    "Step 4:\n",
    " The prepared file is uploaded on our web portal where it is processed and the trained models carry out their intended tasks.\n",
    " \n",
    "Step 5: The final results returned by the model are displayed on the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url of our dataset\n",
    "\n",
    "BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n",
    "\n",
    "\n",
    "def process_feature_text(text):\n",
    "    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n",
    "\n",
    "\n",
    "def prepare_datasets():\n",
    "\n",
    "    #combining our datasets\n",
    "\n",
    "    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n",
    "    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n",
    "    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n",
    "\n",
    "    #initializing our annotation and location arrays\n",
    "\n",
    "    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n",
    "    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n",
    "\n",
    "    merged = df.merge(notes, how=\"left\")\n",
    "    merged = merged.merge(features, how=\"left\")\n",
    "\n",
    "    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n",
    "    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n",
    "    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_list_to_ints(loc_list):\n",
    "    to_return = []\n",
    "    for loc_str in loc_list:\n",
    "        loc_strs = loc_str.split(\";\")\n",
    "        for loc in loc_strs:\n",
    "            start, end = loc.split()\n",
    "            to_return.append((int(start), int(end)))\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def tokenize_and_add_labels(tokenizer, data, config):\n",
    "    out = tokenizer(\n",
    "        data[\"feature_text\"],\n",
    "        data[\"pn_history\"],\n",
    "        truncation=config['truncation'],\n",
    "        max_length=config['max_length'],\n",
    "        padding=config['padding'],\n",
    "        return_offsets_mapping=config['return_offsets_mapping']\n",
    "    )\n",
    "    labels = [0.0] * len(out[\"input_ids\"])\n",
    "    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n",
    "    out[\"sequence_ids\"] = out.sequence_ids()\n",
    "\n",
    "    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n",
    "        if not seq_id or seq_id == 0:\n",
    "            labels[idx] = -1\n",
    "            continue\n",
    "\n",
    "        token_start, token_end = offsets\n",
    "        for feature_start, feature_end in out[\"location_int\"]:\n",
    "            if token_start >= feature_start and token_end <= feature_end:\n",
    "                labels[idx] = 1.0\n",
    "                break\n",
    "\n",
    "    out[\"labels\"] = labels\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n",
    "    all_predictions = []\n",
    "    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n",
    "        pred = 1 / (1 + np.exp(-pred))\n",
    "        start_idx = None\n",
    "        end_idx = None\n",
    "        current_preds = []\n",
    "        words = []\n",
    "        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n",
    "            if seq_id is None or seq_id == 0:\n",
    "                continue\n",
    "\n",
    "            if pred > 0.5:\n",
    "                if start_idx is None:\n",
    "                    start_idx = offset[0]\n",
    "                end_idx = offset[1]\n",
    "            elif start_idx is not None:\n",
    "                if test:\n",
    "                    current_preds.append(f\"{start_idx} {end_idx}\")\n",
    "                    # words.append(test_pn_history[start_idx:end_idx])\n",
    "                else:\n",
    "                    current_preds.append((start_idx, end_idx))\n",
    "                    # words.append(test_pn_history[start_idx:end_idx])\n",
    "                start_idx = None\n",
    "        if test:\n",
    "            all_predictions.append(\"; \".join(current_preds))\n",
    "            # all_predictions.append(\"; \".join(words))\n",
    "        else:\n",
    "            all_predictions.append(current_preds)\n",
    "            # all_predictions.append(\"; \".join(words))\n",
    "            \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n",
    "\n",
    "        num_chars = max(list(chain(*offsets)))\n",
    "        char_labels = np.zeros(num_chars)\n",
    "\n",
    "        for o, s_id, label in zip(offsets, seq_ids, labels):\n",
    "            if s_id is None or s_id == 0:\n",
    "                continue\n",
    "            if int(label) == 1:\n",
    "                char_labels[o[0]:o[1]] = 1\n",
    "\n",
    "        char_preds = np.zeros(num_chars)\n",
    "\n",
    "        for start_idx, end_idx in preds:\n",
    "            char_preds[start_idx:end_idx] = 1\n",
    "\n",
    "        all_labels.extend(char_labels)\n",
    "        all_preds.extend(char_preds)\n",
    "\n",
    "    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"precision\": results[0],\n",
    "        \"recall\": results[1],\n",
    "        \"f1\": results[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text segmentation:\n",
    "\n",
    " The process of splitting written text into meaningful components, such as words, sentences, or subjects, is known as text segmentation. Natural language processing refers to both the mental processes that people utilise while reading text and the artificial processes that are implemented in computers. In our project, the main output is the text segments in the doctor’s notes which have identified particular features present in the patient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Answering: \n",
    "\n",
    "Question answering is a major NLP challenge as well as a long-standing AI milestone. A user may ask a question in plain language and receive an immediate and concise response using QA technologies. The capacity to read a piece of literature and then answer questions about it is known as reading comprehension. Reading comprehension is challenging for computers because it necessitates a combination of natural language comprehension and global knowledge. In our project, the context is provided by the doctor’s notes. The questions queried are the features present in the particular patient and the answer expected is the text segment in which the doctor has identified the query feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n",
    "\n",
    "        input_ids = np.array(tokens[\"input_ids\"])\n",
    "        attention_mask = np.array(tokens[\"attention_mask\"])\n",
    "#         token_type_ids = np.array(tokens[\"token_type_ids\"])\n",
    "\n",
    "        labels = np.array(tokens[\"labels\"])\n",
    "        offset_mapping = np.array(tokens['offset_mapping'])\n",
    "        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n",
    "        \n",
    "        return input_ids, attention_mask, labels, offset_mapping, sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained(config['model_name'])  # BERT model\n",
    "        self.dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.fc1(outputs[0])\n",
    "        logits = self.fc2(self.dropout(logits))\n",
    "        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our hyperparameters\n",
    "hyperparameters = {\n",
    "    \"max_length\": 416,\n",
    "    \"padding\": \"max_length\",\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"truncation\": \"only_second\",\n",
    "    \"model_name\": \"../input/huggingface-roberta-variants/roberta-base/roberta-base\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 1e-5,\n",
    "    \"test_size\": 0.2,\n",
    "    \"seed\": 1268,\n",
    "    \"batch_size\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 11440\n",
      "Test Size 2860\n"
     ]
    }
   ],
   "source": [
    "#splitting the training and testing dataset\n",
    "\n",
    "train_df = prepare_datasets()\n",
    "\n",
    "X_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'],\n",
    "                                   random_state=hyperparameters['seed'])\n",
    "\n",
    "#checking the size\n",
    "print(\"Train size\", len(X_train))\n",
    "print(\"Test Size\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the roberta tokenizer and using\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(hyperparameters['model_name'])\n",
    "\n",
    "#training data on our custom dataset\n",
    "training_data = CustomDataset(X_train, tokenizer, hyperparameters)\n",
    "train_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "\n",
    "#checking the testing data on our custom dataset\n",
    "\n",
    "test_data = CustomDataset(X_test, tokenizer, hyperparameters)\n",
    "test_dataloader = DataLoader(test_data, batch_size=hyperparameters['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta-variants/roberta-base/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CustomModel(hyperparameters).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "#             token_type_ids = batch[2].to(DEVICE)\n",
    "            labels = batch[2].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            # since, we have\n",
    "            loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "            train_loss.append(loss.item() * input_ids.size(0))\n",
    "            loss.backward()\n",
    "            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            # it's also improve f1 accuracy slightly\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        return sum(train_loss)/len(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        offsets = []\n",
    "        seq_ids = []\n",
    "        valid_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "#             token_type_ids = batch[2].to(DEVICE)\n",
    "            labels = batch[2].to(DEVICE)\n",
    "            offset_mapping = batch[3]\n",
    "            sequence_ids = batch[4]\n",
    "\n",
    "        #initializing the loss functions\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "            valid_loss.append(loss.item() * input_ids.size(0))\n",
    "\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "            offsets.append(offset_mapping.numpy())\n",
    "            seq_ids.append(sequence_ids.numpy())\n",
    "            valid_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        #concatenating all the predictions\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        offsets = np.concatenate(offsets, axis=0)\n",
    "        seq_ids = np.concatenate(seq_ids, axis=0)\n",
    "        valid_labels = np.concatenate(valid_labels, axis=0)\n",
    "        location_preds = get_location_predictions(preds, offsets, seq_ids, test=False)\n",
    "        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n",
    "\n",
    "        return sum(valid_loss)/len(valid_loss), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#initializing the variables\n",
    "train_loss_data, valid_loss_data = [], []\n",
    "score_data_list = []\n",
    "valid_loss_min = np.Inf\n",
    "since = time.time()\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17288/1220029216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: {}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# first train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "# training the REBERTA modle for epochs\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(i + 1, epochs))\n",
    "    # first train model\n",
    "    #training the model on teh datastet using optimizer and printing the training loss after every epoch\n",
    "    train_loss = train_model(model, train_dataloader, optimizer, criterion)\n",
    "    train_loss_data.append(train_loss)\n",
    "    print(f\"Train loss: {train_loss}\")\n",
    "    # evaluate model\n",
    "    #calculating the validation lossesa and accuracy\n",
    "    valid_loss, score = eval_model(model, test_dataloader, criterion)\n",
    "    valid_loss_data.append(valid_loss)\n",
    "    score_data_list.append(score)\n",
    "    print(f\"Valid loss: {valid_loss}\")\n",
    "    print(f\"Valid score: {score}\")\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n",
    "\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../input/nbme_bert_v2.pth\", map_location = DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_df():\n",
    "    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n",
    "    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n",
    "    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n",
    "\n",
    "    #merging all the files\n",
    "\n",
    "    merged = test.merge(notes, how = \"left\")\n",
    "    merged = merged.merge(feats, how = \"left\")\n",
    "\n",
    "    def process_feature_text(text):\n",
    "        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n",
    "    \n",
    "    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "class SubmissionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data.loc[idx]\n",
    "        tokenized = self.tokenizer(\n",
    "            example[\"feature_text\"],\n",
    "            example[\"pn_history\"],\n",
    "            truncation = self.config['truncation'],\n",
    "            max_length = self.config['max_length'],\n",
    "            padding = self.config['padding'],\n",
    "            return_offsets_mapping = self.config['return_offsets_mapping']\n",
    "        )\n",
    "        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n",
    "\n",
    "        input_ids = np.array(tokenized[\"input_ids\"])\n",
    "        attention_mask = np.array(tokenized[\"attention_mask\"])\n",
    "        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n",
    "        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n",
    "\n",
    "        return input_ids, attention_mask, offset_mapping, sequence_ids\n",
    "\n",
    "\n",
    "test_df = create_test_df()\n",
    "\n",
    "submission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\n",
    "submission_dataloader = DataLoader(submission_data, batch_size=hyperparameters['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fb61e8953840b799777160b3901020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "offsets = []\n",
    "seq_ids = []\n",
    "\n",
    "for batch in tqdm(submission_dataloader):\n",
    "    input_ids = batch[0].to(DEVICE)\n",
    "    attention_mask = batch[1].to(DEVICE)\n",
    "    offset_mapping = batch[2]\n",
    "    sequence_ids = batch[3]\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    \n",
    "    preds.append(logits.detach().cpu().numpy())\n",
    "    offsets.append(offset_mapping.numpy())\n",
    "    seq_ids.append(sequence_ids.numpy())\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "offsets = np.concatenate(offsets, axis=0)\n",
    "seq_ids = np.concatenate(seq_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling teh getting location function\n",
    "location_preds = get_location_predictions(preds, offsets, seq_ids, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing the length of the predictions\n",
    "len(location_preds), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"location\"] = location_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPI: 17yo M presents with palpitations. Patient reports 3-4 months of intermittent episodes of \"heart beating/pounding out of my chest.\" 2 days ago during a soccer game had an episode, but this time had chest pressure and felt as if he were going to pass out (did not lose conciousness). Of note patient endorses abusing adderall, primarily to study (1-3 times per week). Before recent soccer game, took adderrall night before and morning of game. Denies shortness of breath, diaphoresis, fevers, chills, headache, fatigue, changes in sleep, changes in vision/hearing, abdominal paun, changes in bowel or urinary habits. \n",
      "PMHx: none\n",
      "Rx: uses friends adderrall\n",
      "FHx: mom with \"thyroid disease,\" dad with recent heart attcak\n",
      "All: none\n",
      "Immunizations: up to date\n",
      "SHx: Freshmen in college. Endorses 3-4 drinks 3 nights / week (on weekends), denies tabacco, endorses trying marijuana. Sexually active with girlfriend x 1 year, uses condoms\n"
     ]
    }
   ],
   "source": [
    "#seeing the patient history of a patient\n",
    "test_pn_history = test_df.iloc[0]['pn_history']\n",
    "print(test_pn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>696 724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>668 693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>203 217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>70 91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id location\n",
       "0  00016_000  696 724\n",
       "1  00016_001  668 693\n",
       "2  00016_002  203 217\n",
       "3  00016_003    70 91\n",
       "4  00016_004      NaN"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing or checking the right annotations\n",
    "test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)\n",
    "pd.read_csv(\"submission.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dad with recent heart attcak\n"
     ]
    }
   ],
   "source": [
    "#checking for teh right annotation\n",
    "print(test_pn_history[696:724])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
